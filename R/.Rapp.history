t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
}
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
#Output IPD #
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
IPD
#Find Kaplan-Meier estimates #
IPD<-as.data.frame(IPD) #
KM.est<-survfit(Surv(IPD[,1],IPD[,2])~1,data=IPD,type="kaplan-meier",) #
KM.est #
summary(KM.est)
time = IPD[,1]
status = IPD[,2]
arm = IPD[,3]
IPD
pbo_IPD <- IPD
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 'trt'#
if (arm.id == 'trt') {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
}
arm.id <- 0#
tot.events <- 'NA'#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
} #
#
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
matrix(c(t.S,n.hat[1:n.t],d,cen),ncol=4,byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
trt_IPD <- IPD
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
tot_dat
head(pbo_IPD)
head(trt_IPD)
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 1#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
}#
tot.events <- 'NA'#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}#
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
} #
#
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S,n.hat[1:n.t],d,cen),ncol=4,byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
trt_IPD <- IPD
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 0#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
}#
tot.events <- 'NA'#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}#
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
} #
#
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S,n.hat[1:n.t],d,cen),ncol=4,byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
pbo_IPD <- IPD
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
head(tot_dat)
head(pbo_IPD)
head(trt_IPD)
dim(pbo_IPD)
dim(trt_IPD)
head(arm)
head(IPD)
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 0#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
}#
tot.events <- 'NA'#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}#
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
} #
#
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S,n.hat[1:n.t],d,cen),ncol=4,byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
pbo_IPD <- IPD
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
library(survRM2)#
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=48)
rmst2(time=time, status=status, arm=arm, tau=46.8)
1.2 * 0.9
45.1 + 1.08
44.8 = 1.08
44.8 + 1.08
sum <- 0#
for (i in 2:nrow(digizeit)) {#
	sum <- sum + KM.hat[i-1]*(t.S[i] - t.S[i-1])#
}
sum
tail(IPD)
IPD[2300:2400,]
IPD[2300:2400,1] <- 46
digizeit
IPD[2300:2400,1] <- 46.1
pbo_IPD <- IPD
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=46.8)
arm.id <- 1 #
if (arm.id) <- 1 {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}
arm.id <- 1 #
if (arm.id) <- 1 {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
#Output IPD #
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F) #
#pbo_IPD <- IPD#
trt_IPD <- IPD
arm.id <- 0#
if (arm.id) <- 1 {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S, n.hat[1:n.t], d, cen), ncol=4, byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
arm.id <- 0#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S, n.hat[1:n.t], d, cen), ncol=4, byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
pbo_IPD <- IPD
arm.id <- 0
d('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 0#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}
setwd('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 0#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start th
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}#
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}#
matrix(c(t.S, n.hat[1:n.t], d, cen), ncol=4, byrow=F)#
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}#
#Output IPD
d
sumDL
setwd('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 0#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}
tot.events
sumdL<-sum(d[1:upper[(n.int-1)]])
sumdL
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
setwd('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 0#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}#
#Output IPD #
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
pbo_IPD <- IPD
setwd('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 1#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}
sumdL<-sum(d[1:upper[(n.int-1)]])
sumdL
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
sumdL
sumd
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
trt_IPD <- IPD
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
trt_IPD <- IPD
tot_dat <- rbind(pbo_IPD, trt_IPD)
library(survRM2)#
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=46.8)
head(tot_dat)
tail(pbo_IPD)
tail(trt_IPD)
pbo_extend_rows <- which(pbo_IPD[,1] == 23.7 & pbo_IPD[,2] == 0)
pbo_IPD[pbo_extend_rows, 1] <- 24
trt_extend_rows <- which(trt_IPD[,1] == 23.7 & trt_IPD[,2] == 0)
trt_IPD[trt_extend_rows, 1] <- 24
tail(pbo_IPD)
tail(trt_IPD)
trt_extend_rows <- which(trt_IPD[,1] == 23.8 & trt_IPD[,2] == 0)
trt_IPD[trt_extend_rows, 1] <- 24
setwd('/users/ryansun/dropbox/lj/neratinib_data')#
arm.id <- 1#
if (arm.id == 1) {#
	dt_input <- read.table('neratinib_trt_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_trt_tab2.txt', header=T)#
	tot.events <- 70#
} else {#
	dt_input <- read.table('neratinib_tab1.txt', header=T)#
	pub_risk <- read.table('neratinib_tab2.txt', header=T)#
	tot.events <- 109#
}#
#
# Start their code#
digizeit <- dt_input#
pub.risk <- pub_risk#
#
t.S <- digizeit[, 2]#
S <- digizeit[, 3]#
#
t.risk <- pub.risk[ ,2]#
lower <- pub.risk[, 3]#
upper <- pub.risk[, 4]#
n.risk <- pub.risk[, 5]#
n.int <- length(n.risk)#
n.t <- upper[n.int]#
#
arm <- rep(arm.id, n.risk[1])#
n.censor <- rep(0, (n.int-1)) #
n.hat <- rep(n.risk[1]+1, n.t)#
cen <- rep(0, n.t)#
d <- rep(0, n.t)#
KM.hat <- rep(1, n.t)#
last.i <- rep(1, n.int)#
sumDL <- 0#
#
for (i in 1:(n.int-1)) {#
	# First censoring approximation#
	n.censor[i] <- round(n.risk[i]*S[lower[i+1]]/S[lower[i]] - n.risk[i+1])#
	# Now calculate deaths + censoring for all k within interval i#
	# until n.hat = n.risk at start of interval i+1#
	keep_adjust_censor <- TRUE#
	while (keep_adjust_censor) {#
		if (n.censor[i] <= 0) {#
			cen[lower[i]:upper[i]] <- 0#
			n.censor[i] <- 0#
		}#
		if (n.censor[i] > 0) {#
			cen.t <- rep(0, n.censor[i])#
			# Distribute censoring times#
			for (j in 1:n.censor[i]) {#
				cen.t[j] <- t.S[lower[i]] + j*(t.S[lower[(i+1)]] - t.S[lower[i]]) / (n.censor[i] + 1)#
			}#
			# Find no. censored on each time interval#
			cen[lower[i]:upper[i]] <- hist(cen.t, breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts#
		}#
		# Find no. events and no. at risk on each interval#
		n.hat[lower[i]] <- n.risk[i]#
		last <- last.i[i]#
		for (k in lower[i]:upper[i]) {#
			if (i == 1 & k == lower[i]) {#
				d[k] <- 0#
				KM.hat[k] <- 1#
			} else {#
				d[k] <- round(n.hat[k] * (1-S[k]/KM.hat[last]))#
				KM.hat[k] <- KM.hat[last] * (1-(d[k]/n.hat[k]))#
			}#
			n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
			if (d[k] != 0) {last <- k}#
		}#
		n.censor[i] <- n.censor[i] + (n.hat[lower[i+1]] - n.risk[i+1])#
		if (n.hat[lower[i+1]]  == n.risk[i+1]) {#
			keep_adjust_censor <- FALSE#
		}#
		if (n.hat[lower[i+1]] < n.risk[i+1] & n.censor[i] <= 0) {#
			keep_adjust_censor <- FALSE#
		}#
	}#
	if (n.hat[lower[i+1]] < n.risk[i+1]) {#
		n.risk[i+1] <- n.hat[lower[i+1]]#
	}#
	last.i[i+1] <- last#
}#
#
# Take care of censoring on very last interval#
if (n.int > 1) {#
	n.censor[n.int] <- min(round(sum(n.censor[1:(n.int-1)]*(t.S[upper[n.int]]-t.S[lower[n.int]]) / #
							t.S[upper[(n.int-1)]] - t.S[lower[1]])), n.risk[n.int])#
}#
if (n.int == 1) {#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] <= 0) {#
	cen[lower[n.int]:(upper[n.int]-1)] <- 0#
	n.censor[n.int] <- 0#
}#
if (n.censor[n.int] > 0) {#
	cen.t <- rep(0, n.censor[n.int])#
	for (j in 1:n.censor[n.int]) {#
		cen.t[j] <- t.S[lower[n.int]] + #
			j*(t.S[upper[n.int]] - t.S[lower[n.int]]) / (n.censor[n.int]+1)#
	}#
	cen[lower[n.int]:(upper[n.int]-1)] <- hist(cen.t, breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts#
}#
#
# Find no. events and no. at risk for last interval#
n.hat[lower[n.int]] <- n.risk[n.int]#
last <- last.i[n.int]#
for (k in lower[n.int]:upper[n.int]) {#
	if (KM.hat[last] != 0) {#
		d[k] <- round(n.hat[k]*(1-(S[k] / KM.hat[last])))#
	} else {#
		d[k] <- 0#
	}#
	KM.hat[k] <- KM.hat[last] * (1-(d[k] / n.hat[k]))#
	n.hat[k+1] <- n.hat[k] - d[k] - cen[k]#
	# No. at risk cannot be negative#
	if (n.hat[k+1] < 0) {#
		n.hat[k+1] <- 0#
		cen[k] <- n.hat[k] - d[k]#
	}#
	if (d[k] != 0) {last <- k}#
}
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
trt_IPD <- IPD
trt_extend_rows <- which(trt_IPD[,1] == 23.8 & trt_IPD[,2] == 0)
trt_IPD[trt_extend_rows, 1] <- 24
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
tail(pbo_IPD)
tail(trt_IPD)
library(survRM2)#
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=46.8)
rmst2(time=time, status=status, arm=arm, tau=24)
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 0#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
	tot.events <- 171#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
	tot.events <- 210#
}#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}
rm(list=ls())
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 0#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
	tot.events <- 171#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
	tot.events <- 210#
}#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]#
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
}
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}
sumdL<-sum(d[1:upper[(n.int-1)]])
sumdL
tot.events
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
sumd
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
pbo_IPD <- IPD
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 1#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
	tot.events <- 171#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
	tot.events <- 210#
}#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0#
if (n.int > 1){ #
	#Time intervals 1,...,(n.int-1) #
	for (i in 1:(n.int-1)){ #
		#First approximation of no. censored on interval i #
		n.censor[i]<- round(n.risk[i]*S[lower[i+1]]/S[lower[i]]- n.risk[i+1]) #
		#Adjust tot. no. censored until n.hat = n.risk at start of interval (i+1) #
		while((n.hat[lower[i+1]]>n.risk[i+1]) || ((n.hat[lower[i+1]]<n.risk[i+1])&&(n.censor[i]>0))){ #
			if (n.censor[i]<=0){ #
				cen[lower[i]:upper[i]]<-0 #
				n.censor[i]<-0 #
			} #
			if (n.censor[i]>0){ #
				cen.t<-rep(0,n.censor[i]) #
				for (j in 1:n.censor[i]){ #
					cen.t[j]<- t.S[lower[i]] + j*(t.S[lower[(i+1)]]-t.S[lower[i]])/(n.censor[i]+1) #
				} #
				#Distribute censored observations evenly over time. Find no. censored on each time interval.#
				cen[lower[i]:upper[i]]<-hist(cen.t,breaks=t.S[lower[i]:lower[(i+1)]], plot=F)$counts #
			} #
			#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves#
			n.hat[lower[i]]<-n.risk[i] #
			last<-last.i[i] #
			for (k in lower[i]:upper[i]){ #
				if (i==1 & k==lower[i]){ #
					d[k]<-0 #
					KM.hat[k]<-1 #
				} #
				else { #
					d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
					KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				} #
				n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
				if (d[k] != 0) {last<-k}#
			} #
			n.censor[i]<- n.censor[i]+(n.hat[lower[i+1]]-n.risk[i+1]) #
		} #
		if (n.hat[lower[i+1]]<n.risk[i+1]) {n.risk[i+1] <- n.hat[lower[i+1]]}#
		last.i[i+1] <- last#
	}#
}
#Time interval n.int. #
if (n.int>1){ #
	#Assume same censor rate as average over previous time intervals. #
	n.censor[n.int]<- min(round(sum(n.censor[1:(n.int-1)])*(t.S[upper[n.int]]- #
	t.S[lower[n.int]])/(t.S[upper[(n.int-1)]]-t.S[lower[1]])), n.risk[n.int]) #
} #
if (n.int==1){n.censor[n.int]<-0} #
if (n.censor[n.int] <= 0){ #
	cen[lower[n.int]:(upper[n.int]-1)]<-0 #
	n.censor[n.int]<-0 #
} #
if (n.censor[n.int]>0){ #
	cen.t<-rep(0,n.censor[n.int]) #
	for (j in 1:n.censor[n.int]){ #
		cen.t[j]<- t.S[lower[n.int]] + #
		j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
	} #
	cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
} #
#
#Find no. events and no. at risk on each interval to agree with K-M estimates read from curves #
n.hat[lower[n.int]]<-n.risk[n.int] #
last<-last.i[n.int] #
for (k in lower[n.int]:upper[n.int]){ #
	if(KM.hat[last] !=0){ #
		d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last])))#
	} else {d[k]<-0} #
	KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
	n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
	#No. at risk cannot be negative #
	if (n.hat[k+1] < 0) { #
		n.hat[k+1]<-0 #
		cen[k]<-n.hat[k] - d[k] #
	} #
	if (d[k] != 0) {last<-k }#
}
sumdL<-sum(d[1:upper[(n.int-1)]])
tot.events
sumdL
#If total no. of events reported, adjust no. censored so that total no. of events agrees. #
if (tot.events != "NA"){ #
	if (n.int>1){ #
		sumdL<-sum(d[1:upper[(n.int-1)]]) #
		#If total no. events already too big, then set events and censoring = 0 on all further time intervals #
		if (sumdL >= tot.events){ #
			d[lower[n.int]:upper[n.int]]<- rep(0,(upper[n.int]-lower[n.int]+1)) #
			cen[lower[n.int]:(upper[n.int]-1)]<- rep(0,(upper[n.int]-lower[n.int])) #
			n.hat[(lower[n.int]+1):(upper[n.int]+1)]<- rep(n.risk[n.int],(upper[n.int]+1-lower[n.int])) #
		} #
	} #
	#Otherwise adjust no. censored to give correct total no. events #
	if ((sumdL < tot.events)|| (n.int==1)){ #
		sumd<-sum(d[1:upper[n.int]]) #
		while ((sumd > tot.events)||((sumd< tot.events)&&(n.censor[n.int]>0))){ #
			n.censor[n.int]<- n.censor[n.int] + (sumd - tot.events) #
			if (n.censor[n.int]<=0){ #
				cen[lower[n.int]:(upper[n.int]-1)]<-0 #
				n.censor[n.int]<-0 #
			} #
			if (n.censor[n.int]>0){ #
				cen.t<-rep(0,n.censor[n.int]) #
				for (j in 1:n.censor[n.int]){ #
					cen.t[j]<- t.S[lower[n.int]] + j*(t.S[upper[n.int]]-t.S[lower[n.int]])/(n.censor[n.int]+1) #
				} #
				cen[lower[n.int]:(upper[n.int]-1)]<-hist(cen.t,breaks=t.S[lower[n.int]:upper[n.int]], plot=F)$counts #
			} #
			n.hat[lower[n.int]]<-n.risk[n.int] #
			last<-last.i[n.int] #
			for (k in lower[n.int]:upper[n.int]){ #
				d[k]<-round(n.hat[k]*(1-(S[k]/KM.hat[last]))) #
				KM.hat[k]<-KM.hat[last]*(1-(d[k]/n.hat[k])) #
				if (k != upper[n.int]){ #
					n.hat[k+1]<-n.hat[k]-d[k]-cen[k] #
					#No. at risk cannot be negative #
					if (n.hat[k+1] < 0) { #
						n.hat[k+1]<-0 #
						cen[k]<-n.hat[k] - d[k] #
					} #
				} #
				if (d[k] != 0) {last<-k} #
			} #
			sumd<- sum(d[1:upper[n.int]]) #
		} #
	}#
}
sumd
t.IPD<-rep(t.S[n.t],n.risk[1]) #
event.IPD<-rep(0,n.risk[1])#
#Write event time and event indicator (=1) for each event, as separate row in t.IPD and event.IPD #
k=1#
 for (j in 1:n.t){ #
  	if(d[j]!=0){ #
  		t.IPD[k:(k+d[j]-1)]<- rep(t.S[j],d[j]) #
  		event.IPD[k:(k+d[j]-1)]<- rep(1,d[j]) #
  		k<-k+d[j] #
  	} #
} #
#Write censor time and event indicator (=0) for each censor, as separate row in t.IPD and event.IPD #
for (j in 1:(n.t-1)){ #
  	if(cen[j]!=0){ #
  		t.IPD[k:(k+cen[j]-1)]<- rep(((t.S[j]+t.S[j+1])/2),cen[j]) #
  		event.IPD[k:(k+cen[j]-1)]<- rep(0,cen[j])#
  		k<-k+cen[j] #
  	} #
}
IPD<-matrix(c(t.IPD,event.IPD,arm),ncol=3,byrow=F)
trt_IPD <- IPD
tot_dat <- rbind(pbo_IPD, trt_IPD)
dim(tot_dat)
library(survRM2)#
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=46.8)
tail(trt_IPD)
tail(pbo_IPD)
which(pbo_IPD[,1] == 46.8 & pbo_IPD[,2] == 1)
which(trt_IPD[,1] == 47.8 & trt_IPD[,2] == 1)
trt_extend_rows <- which(trt_IPD[,1] == 47.8 & pbo_IPD[,2] == 0)
pbo_extend_rows <- which(pbo_IPD[,1] == 46.8 & pbo_IPD[,2] == 0)
trt_extend_rows <- which(trt_IPD[,1] == 47.8 & pbo_IPD[,2] == 0)
trt_extend_rows <- which(trt_IPD[,1] == 47.8 & trt_IPD[,2] == 0)
# Replace censoring times at 46.8 (trt) and 46.8 (pbo) with 48#
extend_pbo_IPD <- pbo_IPD#
pbo_extend_rows <- which(extend_pbo_IPD[,1] == 46.8 & extend_pbo_IPD[,2] == 0)#
extend_pbo_IPD[pbo_extend_rows, 1] <- 48#
#
extend_trt_IPD <- trt_IPD#
trt_extend_rows <- which(extend_trt_IPD[,1] == 47.8 & extend_trt_IPD[,2] == 0)#
extend_trt_IPD[trt_extend_rows, 1] <- 48#
#
tot_dat <- rbind(extend_pbo_IPD, extend_trt_IPD)
library(survRM2)#
time <- tot_dat[,1]#
status <- tot_dat[,2]#
arm <- tot_dat[,3]#
rmst2(time=time, status=status, arm=arm, tau=46.8)
rmst2(time=time, status=status, arm=arm, tau=48)
sum <- 0#
for (i in 2:nrow(digizeit)) {#
	sum <- sum + KM.hat[i-1]*(t.S[i] - t.S[i-1])#
}
sum
sum <- sum + (48 - t.S[length(t.S)]) * KM.hat[length(KM.hat)]
sum
# Read tables 1 and 2#
setwd('/users/ryansun/dropbox/lj/pertuzumab_data')#
arm.id <- 1#
if (arm.id == 1) {#
	digizeit <- read.table('permutanab_trt_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_trt_tab2.txt', header=T)#
	tot.events <- 171#
} else {#
	digizeit <- read.table('permutanab_pbo_tab1.txt', header=T)#
	pub.risk <- read.table('permutanab_pbo_tab2.txt', header=T)#
	tot.events <- 210#
}#
t.S<-digizeit[,2] #
S<-digizeit[,3]#
#
t.risk<-pub.risk[,2] #
lower<-pub.risk[,3] #
upper<-pub.risk[,4] #
n.risk<-pub.risk[,5] #
n.int<-length(n.risk) #
n.t<- upper[n.int]
n.t
n.int
upper
upper
pub_risk
pub.risk
arm<-rep(arm.id,n.risk[1]) #
n.censor<- rep(0,(n.int-1)) #
n.hat<-rep(n.risk[1]+1,n.t) #
cen<-rep(0,n.t) #
d<-rep(0,n.t) #
KM.hat<-rep(1,n.t)#
last.i<-rep(1,n.int) #
sumdL<-0
n.hat
n.risk
n.t
n.t
n_risk
?seq
ell = 1:10
quantiles(ell)
quantile(ell)
quantile(ell, c(1/3, 2/3))
exp(0.1) / exp(1.1)
exp(0.1)
setwd('/users/ryansun/documents/research/paper2/software/geint/R')#
source('GE_bias_normal_squaredmis.R')#
source('GE_bias.R')#
source('GE_enumerate_inputs.R')#
source('GE_nleqslv.R')#
source('GE_scoreeq_sim.R')#
source('GE_test_moment_calcs.R')#
source('GE_translate_inputs.R')
##########################################################################
####################################################################################
####################################################################################
# Test a result of the paper, that if we have GE independence, \beta_I=0, and each#
# Z and W is independent of at least one of G or E, then \alpha_I = 0.#
# Make the first half of Zs and Ws independent of G, and the second half of#
# them independent of E.#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
half_Z <- ceiling(num_Z/2)#
half_W <- ceiling(num_W/2)#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( 0,#
					 c( rep(0, half_Z), runif(n=(num_Z-half_Z), min=0.1, max=0.3) ),#
					  c( runif(n=half_Z, min=0.1, max=0.3), rep(0, num_Z-half_Z) ),#
					   c( rep(0, half_W), runif(n=(num_W-half_W), min=0.1, max=0.3) ),#
					    c( runif(n=half_W, min=0.1, max=0.3), rep(0, num_W-half_W) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}
# Show the parameters#
beta_list#
prob_G
# Test all functions#
# First make normal + squared_mis assumptions#
GE_test_moment_calcs(beta_list, rho_list, prob_G, cov_Z, cov_W, num_sub=2000000, test_threshold=0.003)#
sim_results <- GE_scoreeq_sim(beta_list=beta_list, prob_G=prob_G, rho_list=rho_list, cov_Z=cov_Z, cov_W=cov_W)#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
#
# Record higher order moments#
cov_list <- solve_results$cov_list#
HOM_list <- solve_results$HOM_list#
cov_mat_list <- solve_results$cov_mat_list#
mu_list <- solve_results$mu_list
# Make the first half of Zs and Ws independent of G, and the second half of#
# them independent of E.#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
half_Z <- ceiling(num_Z/2)#
half_W <- ceiling(num_W/2)#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( 0,#
					 c( rep(0, half_Z), runif(n=(num_Z-half_Z), min=0.1, max=0.3) ),#
					  c( runif(n=half_Z, min=0.1, max=0.3), rep(0, num_Z-half_Z) ),#
					   c( rep(0, half_W), runif(n=(num_W-half_W), min=0.1, max=0.3) ),#
					    c( runif(n=half_W, min=0.1, max=0.3), rep(0, num_W-half_W) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
GE_test_moment_calcs(beta_list, rho_list, prob_G, cov_Z, cov_W, num_sub=2000000, test_threshold=0.003)#
sim_results <- GE_scoreeq_sim(beta_list=beta_list, prob_G=prob_G, rho_list=rho_list, cov_Z=cov_Z, cov_W=cov_W)#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)
# Record higher order moments#
cov_list <- solve_results$cov_list#
HOM_list <- solve_results$HOM_list#
cov_mat_list <- solve_results$cov_mat_list#
mu_list <- solve_results$mu_list#
#
# Generic test functions#
nleqslv_results <- GE_nleqslv(beta_list, cov_list, cov_mat_list, mu_list, HOM_list)#
ge_bias_results <- GE_bias(beta_list, cov_list, cov_mat_list, mu_list, HOM_list)
nleqslv_results$x#
sim_results#
unlist(solve_results$alpha_list)#
unlist(ge_bias_results)#
nleqslv_results$x - sim_results#
nleqslv_results$x - unlist(solve_results$alpha_list)#
nleqslv_results$x - unlist(ge_bias_results)
###################################################################
#####################################################################################
#####################################################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
###############################################################
#####################################################################################
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_G - only \alpha_G changes#
beta_list[[2]] <- beta_list[[2]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_E - both \alpha_G and \alpha_I change#
beta_list[[3]] <- beta_list[[3]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_I - both \alpha_G and \alpha_I change#
beta_list[[4]] <- beta_list[[4]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_Z - Nothing changes????#
beta_list[[5]] <- beta_list[[5]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_M - Both change#
beta_list[[6]] <- beta_list[[6]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_G - only \alpha_G changes#
beta_list[[2]] <- beta_list[[2]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Now change beta_E - both \alpha_G and \alpha_I change#
beta_list[[3]] <- beta_list[[3]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]#
#
# Now change beta_I - both \alpha_G and \alpha_I change#
beta_list[[4]] <- beta_list[[4]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]#
#
# Now change beta_Z - Nothing changes????#
beta_list[[5]] <- beta_list[[5]] * 1.5#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Test GE_BICS()#
set.seed(100)#
n <- 500#
Y <- rnorm(n=n)#
E <- rnorm(n=n)#
G <- rbinom(n=n, size=2, prob=0.3)#
design_mat <- cbind(1, G, E, G*E)#
GE_BICS(outcome=Y, design_mat=design_mat, desired_coef=4, outcome_type='D')
####################################################################################
####################################################################################
####################################################################################
####################################################################################
# Test a result of the paper, that if we have GE independence, \beta_I=0, and each#
# Z and W is independent of at least one of G or E, then \alpha_I = 0.#
# Make the first half of Zs and Ws independent of G, and the second half of#
# them independent of E.#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
half_Z <- ceiling(num_Z/2)#
half_W <- ceiling(num_W/2)#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( 0,#
					 c( rep(0, half_Z), runif(n=(num_Z-half_Z), min=0.1, max=0.3) ),#
					  c( runif(n=half_Z, min=0.1, max=0.3), rep(0, num_Z-half_Z) ),#
					   c( rep(0, half_W), runif(n=(num_W-half_W), min=0.1, max=0.3) ),#
					    c( runif(n=half_W, min=0.1, max=0.3), rep(0, num_W-half_W) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )
beta_list
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G
sim_results <- GE_scoreeq_sim(beta_list=beta_list, prob_G=prob_G, rho_list=rho_list, cov_Z=cov_Z, cov_W=cov_W)#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
#
# Record higher order moments#
cov_list <- solve_results$cov_list#
HOM_list <- solve_results$HOM_list#
cov_mat_list <- solve_results$cov_mat_list#
mu_list <- solve_results$mu_list#
#
# Generic test functions#
nleqslv_results <- GE_nleqslv(beta_list, cov_list, cov_mat_list, mu_list, HOM_list)#
ge_bias_results <- GE_bias(beta_list, cov_list, cov_mat_list, mu_list, HOM_list)#
#
# Should all match#
nleqslv_results$x#
sim_results#
unlist(solve_results$alpha_list)#
unlist(ge_bias_results)#
nleqslv_results$x - sim_results#
nleqslv_results$x - unlist(solve_results$alpha_list)#
nleqslv_results$x - unlist(ge_bias_results)
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
set.seed90
set.seed(0)
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
set.seed(0)
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
#' GE_bias_normal_squaredmis_old.R#
#'#
#' A function to calculate the bias in testing for GxE interaction, making many more#
#' assumptions than GE_bias_old().  The additional assumptions are added to simplify the process#
#' of calculating/estimating many higher order moments which the user may not be familiar with. \cr#
#' The following assumptions are made: \cr#
#' (1) All fitted covariates besides G (that is, E, all Z, and all W) have a marginal standard #
#' normal distribution with mean 0 and variance 1.  This corresponds to the case of the researcher#
#' standardizing all of their fitted covariates. \cr#
#' (2) G is generated by means of thresholding two independent normal RVs and is centered to have mean 0.#
#' (3) The joint distributions of E, Z, W, and the thresholded variables underlying G can be described#
#' by a multivariate normal distribution. \cr#
#' (4) The misspecification is of the form f(E)=h(E)=E^2, and M_j=W_j^2 for all j. In particular,#
#' W always has the same length as M here. \cr#
#' #
#' @param beta_list A list of the effect sizes in the true model.#
#' Use the order beta_0, beta_G, beta_E, beta_I, beta_Z, beta_M.#
#' If Z or M is a vector, then beta_Z and beta_M should be vectors.#
#' If Z or M is not in the model (i.e. all covariates other than G+E #
#' have been specified incorrectly, or all covariates other than G+E have been #
#' specified correctly, or the only covariates are G+E), then set beta_Z=0 and/or beta_M=0.#
#' @param rho_list A list of the 6 pairwise covariances between the#
#' covariates.  These should be in the order (1) cov_GE (2) cov_GZ (3) cov_EZ#
#' (4) cov_GW (5) cov_EW (6) cov_ZW.#
#' Again if Z or W are vectors then terms like cov_GZ should be vectors (in the order#
#' cov(G,Z_1),...,cov(G,Z_p)) where Z is of dimension p, and similarly for W.#
#' If Z or M are vectors, then cov_ZW should be a vector in the order (cov(Z_1,W_1),...,cov(Z_1,W_q),#
#' cov(Z_2,W_1),........,cov(Z_p,W_q) where Z is a vector of length p and W is a vector of length q.#
#' If Z or M are not in the model then treat them as the constant 0.#
#' So for example if Z is not in the model and M (and therefore W) is a vector of length 2, we would#
#' have cov_EZ=0 and cov(ZW) = (0,0).#
#' @param cov_Z Only specify this if Z is a vector, gives the covariance matrix of Z (remember by assumption#
#' Z has mean 0 and variance 1).  The (i,j) element of the matrix should be the (i-1)(i-2)/2+j element#
#' of the vector.#
#' @param cov_W Only specify this if W is a vector, gives the covariance matrix of W (remember by assumption#
#' W has mean 0 and variance 1).  The (i,j) element of the matrix should be the (i-1)(i-2)/2+j element#
#' of the vector.#
#' @param prob_G Probability that each allele is equal to 1.  Since each SNP has#
#' two alleles, the expectation of G is 2*prob_G.#
#' #
#' @return A list with the elements:#
#' \item{alpha_list}{The asymptotic values of the fitted coefficients alpha.}#
#' \item{beta_list}{The same beta_list that was given as input.}#
#' \item{cov_list}{The list of all covariances (both input and calculated) for use with GE_nleqslv() #
#'	and GE_bias().}#
#' \item{cov_mat_list}{List of additionally calculated covariance matrices for use with GE_nleqslv()#
#' and GE_bias().}#
#' \item{mu_list}{List of calculated means for f(E), h(E), Z, M, and W for use with GE_nleqslv() #
#' and GE_bias().}#
#' \item{HOM_list}{List of calculated Higher Order Moments for use with GE_nleqslv() and GE_bias().}#
#'#
#' @export#
#' @examples #
#' GE_bias_normal_squaredmis_old( beta_list=as.list(runif(n=6, min=0, max=1)), #
#'							rho_list=as.list(rep(0.3,6)), prob_G=0.3)#
#
GE_bias_normal_squaredmis_old <- function(beta_list, rho_list, prob_G, cov_Z=NULL, cov_W=NULL)#
{#
  # Need survival function.#
  surv <- function(x) {1-pnorm(x)}#
  # Record some initial quantities#
  rho_GE <- rho_list[[1]]; rho_GZ <- rho_list[[2]]; rho_EZ <- rho_list[[3]]#
  rho_GW <- rho_list[[4]]; rho_EW <- rho_list[[5]]; rho_ZW <- rho_list[[6]]#
  w <- qnorm(1-prob_G)					#
  r_GE <- rho_GE / (2*dnorm(w))	#
  r_GZ <- rho_GZ / (2*dnorm(w))#
  r_GW <- rho_GW / (2*dnorm(w))#
  beta_0 <- beta_list[[1]]; beta_G <- beta_list[[2]]; beta_E <- beta_list[[3]]#
  beta_I <- beta_list[[4]]; BETA_Z <- beta_list[[5]]; BETA_M <- beta_list[[6]]#
  # Even if Z or M/W is 0 keep the num at 1.#
  num_W <- length(beta_list[[6]])#
  num_Z <- length(beta_list[[5]])#
  # Some error checking, make sure the covariance matrix is ok#
  translated_inputs <- GE_translate_inputs_old(beta_list, rho_list, prob_G, cov_Z, cov_W)#
  sig_mat <- translated_inputs$sig_mat_total#
  sig_mat_ZZ <- translated_inputs$sig_mat_ZZ#
  sig_mat_WW <- translated_inputs$sig_mat_WW#
  # Check for Z and W.#
  # Some obvious and by assumption means.#
  if (is.null(sig_mat_ZZ)) {#
    MU_Z <- 0#
  } else {#
    MU_Z <- rep(0, num_Z)#
  }#
  if (is.null(sig_mat_WW)) {#
    MU_W <- 0#
    MU_M <- 0#
  } else {#
    MU_M <- rep(1, num_W)		#
    MU_W <- rep(0, num_W)	#
  }#
  # More obvious and by assumption.#
  mu_f <- 1#
  mu_h <- 1#
  # Now calculate other harder, necessary terms that have #
  # been determined by our assumptions + inputs#
  #########################
  # Covariances#
  mu_GE <- rho_GE#
  mu_Gf <- 2*r_GE^2*w*dnorm(w) + 2*surv(w) - 2*prob_G#
  mu_Gh <- mu_Gf#
  mu_GG <- 2*prob_G*(1-prob_G)#
  mu_EE <- 1#
  mu_Ef <- 0#
  MU_GZ <- rho_GZ  	# Vector#
  MU_GW <- rho_GW		# Vector#
  MU_EM <- 	rep(0, num_W)				# Vector, in particular because third moment of W is 0#
  MU_fW <- 	rep(0, num_W)		# Vector#
  MU_EW <- rho_EW			# Vector#
  MU_EZ <- rho_EZ	#
  MU_fZ <- 	rep(0, num_Z)	#
  # Depends on if M exists.#
  if (is.null(sig_mat_WW)) {#
    MU_GM <- 0#
  } else {#
    MU_GM <- 	2*r_GW^2*w*dnorm(w) + 2*surv(w) - 2*prob_G	# Vector, see gen_cor_bin_normal for explanation#
  } #
#
  #########################
  # Matrix covariances#
  # MU_ZW is not the same as MU_WZ because the dimensions of the matrix are not the same!#
  # Remember the covariances in rho_ZW are in the order cov(Z_1,W_1), cov(Z_1,W_2), ..., cov(Z_2,W_1),...#
  MU_ZW <- matrix(data=rho_ZW, nrow=num_Z, ncol=num_W, byrow=TRUE)	# Matrix	 #
  MU_WZ <- t(MU_ZW)#
  MU_ZM <- matrix(data=0, nrow=num_Z, ncol=num_W) 		# Matrix#
  MU_WM <- matrix(data=0, nrow=num_W, ncol=num_W) 			# Matrix#
  MU_ZZ <- sig_mat_ZZ 		# Matrix#
  MU_WW <- sig_mat_WW		# Matrix#
  #########################
  # Higher order moments with G+E#
  # We need as intermediate quantities E[G_1E], E[G_1E^2], E[G_1E^3], E[G_1G_2E], E[G_1G_2E^2], E[G1EZ], #
  # E[G1EW], E[G1EW^2], E[G1WE^2], E[G1ZE^2], E[G1G2E^2]#
  mu_G1_E <- r_GE*dnorm(w)#
  mu_G1_EE <- r_GE^2*w*dnorm(w) + surv(w)#
  mu_G1_EEE <- r_GE^3*w^2*dnorm(w) - r_GE^3*dnorm(w) + 3*r_GE*dnorm(w)#
  # E[G1G2E] requires numerical integration#
  temp_sig <- matrix(data=c(1-r_GE^2, -r_GE^2, -r_GE^2, 1-r_GE^2), nrow=2)#
  f_G1_G2_E <- function(x,w,r_GE) {#
  	x*dnorm(x)*mvtnorm::pmvnorm(lower=c(w,w), upper=c(Inf,Inf), mean=c(r_GE*x, r_GE*x), sigma=temp_sig)#
  }#
  mu_G1_G2_E <- pracma::quadinf(f=f_G1_G2_E, xa=-Inf, xb=Inf, w=w, r_GE=r_GE)$Q[1]#
  # E[G1G2E^2] requires numerical integration#
  temp_sig <- matrix(data=c(1-r_GE^2, -r_GE^2, -r_GE^2, 1-r_GE^2), nrow=2)#
  f_G1_G2_EE <- function(x,w,r_GE) {#
  	x^2*dnorm(x)*mvtnorm::pmvnorm(lower=c(w,w), upper=c(Inf,Inf), mean=c(r_GE*x, r_GE*x), sigma=temp_sig)#
  }#
  mu_G1_G2_EE <- pracma::quadinf(f=f_G1_G2_EE, xa=-Inf, xb=Inf, w=w, r_GE=r_GE)$Q[1]#
  # E[G1G2E^3] requires numerical integration#
  temp_sig <- matrix(data=c(1-r_GE^2, -r_GE^2, -r_GE^2, 1-r_GE^2), nrow=2)#
  f_G1_G2_EEE <- function(x,w,r_GE) {#
  	x^3*dnorm(x)*mvtnorm::pmvnorm(lower=c(w,w), upper=c(Inf,Inf), mean=c(r_GE*x, r_GE*x), sigma=temp_sig)#
  }#
  mu_G1_G2_EEE <- pracma::quadinf(f=f_G1_G2_EEE, xa=-Inf, xb=Inf, w=w, r_GE=r_GE)$Q[1]#
  # See gen_cor_bin_normal to see how to do these#
  mu_GGE <- 2*mu_G1_E + 2*mu_G1_G2_E - 8*prob_G*mu_G1_E#
  mu_GGh <- 2*mu_G1_EE + 2*mu_G1_G2_EE + 4*prob_G^2*1 - 8*prob_G*mu_G1_EE#
  mu_GEE <- mu_Gf#
  mu_GEf <- 2*(r_GE^3*w^2*dnorm(w) - r_GE^3*dnorm(w) + 3*r_GE*dnorm(w))#
  mu_GEh <- mu_GEf#
  mu_GGEE <- 2*mu_G1_EE + 2*mu_G1_G2_EE + 4*prob_G^2*1 - 8*prob_G*mu_G1_EE#
  mu_GGEf <- 2*mu_G1_EEE + 2*mu_G1_G2_EEE + 4*prob_G^2*0 - 8*prob_G*mu_G1_EEE#
  mu_GGEh <- mu_GGEf#
  ###########################################
  # Harder ones involving Z and W#
  # E[G1EZ] requires numerical integration#
  f_G1_E_Z <- function(x, w, r_EZ, r_GE, r_GZ) {#
  	( r_EZ * x * surv( (w-x*r_GE) / sqrt(1-r_GE^2) ) + dnorm( (w-r_GE*x) / sqrt(1-r_GE^2) ) * #
  		(r_GZ-r_GE*r_EZ) / sqrt(1-r_GE^2) ) * x* dnorm(x)#
  }#
  if (is.null(sig_mat_ZZ)) {#
    mu_G1_E_Z <- 0#
  } else {#
    mu_G1_E_Z <- rep(NA, num_Z)#
    for (i in 1:num_Z) {#
  	  mu_G1_E_Z[i] <- pracma::quadinf(f= f_G1_E_Z, xa=-Inf, xb=Inf, w=w, r_EZ=rho_EZ[i], r_GE=r_GE, r_GZ=r_GZ[i])$Q#
    }#
  }#
  # E[G1EW] requires numerical integration#
  f_G1_E_W <- function(x, w, r_EW, r_GE, r_GW) {#
  	( r_EW * x * surv( (w-x*r_GE) / sqrt(1-r_GE^2) ) + dnorm( (w-r_GE*x) / sqrt(1-r_GE^2) ) * #
  		(r_GW-r_GE*r_EW) / sqrt(1-r_GE^2) ) * x* dnorm(x)#
  }#
  if (is.null(sig_mat_WW)) {#
    mu_G1_E_W <- 0#
  } else {#
    mu_G1_E_W <- rep(NA, num_W)#
    for (i in 1:num_W) {#
  	  mu_G1_E_W[i] <- pracma::quadinf(f= f_G1_E_W, xa=-Inf, xb=Inf, w=w, r_EW=rho_EW[i], r_GE=r_GE, r_GW=r_GW[i])$Q#
    }#
  }#
  # E[G1EW^2] requires numerical integration#
  f_G1_E_WW <- function(x, w, r_GE, r_GW, r_EW) {#
  	( r_EW * x* surv( (w-x*r_GW) / sqrt(1-r_GW^2) ) + dnorm( (w-r_GW*x) / #
  		sqrt(1-r_GW^2) ) * (r_GE-r_GW*r_EW) / sqrt(1-r_GW^2) ) * x^2 * dnorm(x)#
  }#
  if (is.null(sig_mat_WW)) {#
    mu_G1_E_WW <- 0#
  } else {#
    mu_G1_E_WW <- rep(NA, num_W)#
    for (i in 1:num_W) {#
  	  mu_G1_E_WW[i] <- pracma::quadinf(f=f_G1_E_WW, xa=-Inf, xb=Inf, w=w , r_GE=r_GE, r_GW=r_GW[i], r_EW=rho_EW[i])$Q#
    }#
  }#
  # E[G1WE^2] requires numerical integration#
  f_G1_W_EE <- function(x, w, r_GE, r_GW, r_EW) {#
  	( r_EW * x* surv( (w-x*r_GE) / sqrt(1-r_GE^2) ) + dnorm( (w-r_GE*x) / #
  		sqrt(1-r_GE^2) ) * (r_GW-r_GE*r_EW) / sqrt(1-r_GE^2) ) * x^2 * dnorm(x)#
  }#
  if (is.null(sig_mat_WW)) {#
    mu_G1_W_EE <- 0#
  } else {#
    mu_G1_W_EE <- rep(NA, num_W)#
    for (i in 1:num_W) {#
  	  mu_G1_W_EE[i] <- pracma::quadinf(f=f_G1_W_EE, xa=-Inf, xb=Inf, w=w, r_GE=r_GE, r_GW=r_GW[i], r_EW=rho_EW[i])$Q#
    }#
  }#
  # E[G1ZE^2] requires numerical integration#
  f_G1_Z_EE <- function(x, w, r_GE, r_GZ, r_EZ) {#
  	( r_EZ * x* surv( (w-x*r_GE) / sqrt(1-r_GE^2) ) + dnorm( (w-r_GE*x) / #
  		sqrt(1-r_GE^2) ) * (r_GZ-r_GE*r_EZ) / sqrt(1-r_GE^2) ) * x^2 * dnorm(x)#
  }#
  if (is.null(sig_mat_ZZ)) {#
    mu_G1_Z_EE <- 0#
  } else {#
    mu_G1_Z_EE <- rep(NA, num_Z)#
    for (i in 1:num_Z) {#
  	  mu_G1_Z_EE[i] <- pracma::quadinf(f=f_G1_Z_EE, xa=-Inf, xb=Inf, w=w, r_GE=r_GE, r_GZ=r_GZ[i], r_EZ=rho_EZ[i])$Q#
    }#
  }#
#
  # See gen_cor_bin_normal to see how to do these (vectors)#
  MU_GEZ <- 2*mu_G1_E_Z - 2*prob_G*rho_EZ			# Vector#
  MU_GEW <- 2*mu_G1_E_W	- 2*prob_G*rho_EW			# Vector#
  MU_GEM <-	2*mu_G1_E_WW							# Vector#
  MU_GhW <- 2*mu_G1_W_EE#
  MU_GhZ <- 2*mu_G1_Z_EE#
  #########################
  # Some shortcut quantities#
  A <- (mu_GE * MU_GZ / mu_GG - MU_EZ) / (mu_EE - mu_GE^2/mu_GG)#
  B <- (mu_GE * MU_GW / mu_GG - MU_EW) / (mu_EE - mu_GE^2/mu_GG)#
  # O will be set to 0 if no Z#
  if (is.null(MU_ZZ)) {#
    O <- 0#
    solve_O <- 0#
  } else {#
    O <- MU_Z%*%t(MU_Z) + MU_GZ%*%t(MU_GZ)/mu_GG - MU_ZZ - A %*% t(MU_EZ - MU_GZ*mu_GE/mu_GG)#
    solve_O <- solve(O)#
  }#
  C <- (B %*% t(MU_EZ - MU_GZ*mu_GE/mu_GG) - MU_W%*%t(MU_Z) - MU_GW%*%t(MU_GZ)/mu_GG + MU_WZ) %*% solve_O#
  # Q will be 0 if no W#
  if ( is.null(MU_WW) ) {#
    Q <- 0#
    solve_Q <- 0#
  } else {#
    Q <- MU_W%*%t(MU_W) + MU_GW%*%t(MU_GW)/mu_GG - MU_WW + B %*% t(MU_GW*mu_GE/mu_GG - MU_EW) + #
      C %*% ( MU_Z%*%t(MU_W) + MU_GZ%*%t(MU_GW)/mu_GG - MU_ZW + A %*% t(MU_GW*mu_GE/mu_GG - MU_EW) )#
    solve_Q <- solve(Q)#
  }#
  D <- (mu_GE * mu_GGE / mu_GG - mu_GEE) / (mu_EE - mu_GE^2 / mu_GG)#
  E <- t(MU_GEZ - MU_Z*mu_GE - MU_GZ*mu_GGE/mu_GG + D*(MU_EZ - MU_GZ*mu_GE/mu_GG)) %*% solve_O#
  EFF <- ( t(MU_W*mu_GE + MU_GW*mu_GGE/mu_GG - MU_GEW + D*(MU_GW * mu_GE / mu_GG - MU_EW)) + #
             E %*% (A %*% t(MU_GW*mu_GE/mu_GG - MU_EW) + MU_Z%*%t(MU_W) + MU_GZ%*%t(MU_GW)/mu_GG - MU_ZW) ) %*% solve_Q#
  # Solve for \alpha_I#
  alpha_I_num <- beta_E * (-mu_f*mu_GE - mu_Gf*mu_GGE/mu_GG + mu_GEf + D * (mu_Ef - mu_Gf*mu_GE/mu_GG)) +#
    beta_E * E %*% (-mu_f*MU_Z - MU_GZ*mu_Gf/mu_GG + MU_fZ + A * (mu_Ef - mu_Gf*mu_GE/mu_GG)) + #
    beta_I * (-mu_Gh*mu_GE - mu_GGh*mu_GGE/mu_GG + mu_GGEh + D * (mu_GEh - mu_GGh*mu_GE/mu_GG)) + #
    beta_I * E %*% (MU_GhZ -mu_Gh*MU_Z - MU_GZ*mu_GGh/mu_GG + A * (mu_GEh - mu_GGh*mu_GE/mu_GG)) + #
    t(MU_GEM - MU_M*mu_GE - MU_GM*mu_GGE/mu_GG + D * (MU_EM - MU_GM*mu_GE/mu_GG)) %*% BETA_M + #
    E %*% (A %*% t(MU_EM - MU_GM*mu_GE/mu_GG) - MU_Z%*%t(MU_M) - MU_GZ%*%t(MU_GM)/mu_GG + MU_ZM) %*% BETA_M - #
    beta_E * EFF %*% (-mu_f*MU_W - MU_GW*mu_Gf/mu_GG + MU_fW + B %*% as.matrix(mu_Ef - mu_Gf*mu_GE/mu_GG)) - #
    beta_E * EFF %*% C %*% (-mu_f*MU_Z - mu_Gf*MU_GZ/mu_GG + MU_fZ + A %*% as.matrix(mu_Ef - mu_Gf*mu_GE/mu_GG)) - #
    beta_I * EFF %*% (-mu_Gh*MU_W - MU_GW*mu_GGh/mu_GG + MU_GhW + B %*% as.matrix(mu_GEh - mu_GGh*mu_GE/mu_GG)) -#
    beta_I * EFF %*% C %*% (MU_GhZ - MU_Z*mu_Gh - MU_GZ*mu_GGh/mu_GG + A %*% as.matrix(mu_GEh - mu_GGh*mu_GE/mu_GG)) - #
    EFF %*% ( -MU_W%*%t(MU_M) - MU_GW%*%t(MU_GM)/mu_GG + MU_WM + B %*% t(MU_EM - MU_GM*mu_GE/mu_GG) ) %*% BETA_M - #
    EFF %*% C %*% ( A %*% t(MU_EM - MU_GM*mu_GE/mu_GG) - MU_Z%*%t(MU_M) - MU_GZ%*%t(MU_GM)/mu_GG + MU_ZM) %*% BETA_M#
  alpha_I_denom <- EFF %*% ( MU_W*mu_GE + MU_GW*mu_GGE/mu_GG - MU_GEW + B * (mu_GGE*mu_GE/mu_GG - mu_GEE) ) +#
    EFF %*% C %*% ( MU_Z*mu_GE + MU_GZ*mu_GGE/mu_GG - MU_GEZ + A * (mu_GGE*mu_GE/mu_GG - mu_GEE) ) - #
    ( mu_GE^2 + mu_GGE^2/mu_GG - mu_GGEE + D * (mu_GGE*mu_GE/mu_GG - mu_GEE) ) - #
    E %*% ( MU_Z*mu_GE + MU_GZ*mu_GGE/mu_GG - MU_GEZ + A * (mu_GGE*mu_GE/mu_GG - mu_GEE) )#
  alpha_I <- alpha_I_num / alpha_I_denom#
  R <- beta_E * (-MU_W*mu_f - MU_GW*mu_Gf/mu_GG + MU_fW + B * (mu_Ef - mu_Gf*mu_GE/mu_GG)) + #
    beta_E * C %*% (-mu_f*MU_Z - MU_GZ*mu_Gf/mu_GG + MU_fZ + A %*% as.matrix(mu_Ef - mu_Gf*mu_GE/mu_GG)) + #
    beta_I * (-MU_W*mu_Gh - MU_GW*mu_GGh/mu_GG + MU_GhW + B * (mu_GEh - mu_GGh*mu_GE/mu_GG)) + #
    beta_I * C %*% (MU_GhZ - MU_Z*mu_Gh - MU_GZ*mu_GGh/mu_GG + A * (mu_GEh - mu_GGh*mu_GE/mu_GG)) + #
    ( B %*% t(MU_EM - MU_GM*mu_GE/mu_GG) - MU_W%*%t(MU_M) - MU_GW%*%t(MU_GM)/mu_GG + MU_WM) %*% BETA_M + #
    C %*% ( A %*% t(MU_EM - MU_GM*mu_GE/mu_GG) - MU_Z%*%t(MU_M) - MU_GZ%*%t(MU_GM)/mu_GG + MU_ZM) %*% BETA_M + #
    alpha_I * (MU_W*mu_GE + MU_GW*mu_GGE/mu_GG - MU_GEW + B * (mu_GGE*mu_GE/mu_GG - mu_GEE)) + #
    as.numeric(alpha_I) * C %*% (MU_Z*mu_GE + MU_GZ*mu_GGE/mu_GG - MU_GEZ + A*(mu_GGE*mu_GE/mu_GG - mu_GEE))#
  ALPHA_W <- - solve_Q %*% R#
  P <- beta_E * (-MU_Z*mu_f - MU_GZ*mu_Gf/mu_GG + MU_fZ + A * (mu_Ef - mu_Gf*mu_GE/mu_GG)) + #
    beta_I * (MU_GhZ - MU_Z*mu_Gh - MU_GZ*mu_GGh/mu_GG + A * (mu_GEh - mu_GGh*mu_GE/mu_GG)) + #
    alpha_I * (MU_Z*mu_GE + MU_GZ*mu_GGE/mu_GG - MU_GEZ + A * (mu_GGE*mu_GE/mu_GG - mu_GEE)) + #
    ( A %*% t(MU_EM - MU_GM*mu_GE/mu_GG) - MU_Z%*%t(MU_M) - MU_GZ%*%t(MU_GM)/mu_GG + MU_ZM) %*% BETA_M + #
    ( A %*% t(MU_GW*mu_GE/mu_GG - MU_EW) + MU_Z%*%t(MU_W) + MU_GZ%*%t(MU_GW)/mu_GG - MU_ZW) %*% ALPHA_W#
  Bz_Az <- solve_O %*% P#
  ALPHA_Z <- BETA_Z - solve_O %*% P#
  alpha_E <- ( beta_E * (mu_Ef - mu_Gf*mu_GE/mu_GG) + beta_I * (mu_GEh - mu_GGh*mu_GE/mu_GG) +  #
                 alpha_I * (mu_GGE*mu_GE/mu_GG - mu_GEE) + t(MU_EZ - MU_GZ*mu_GE/mu_GG) %*% Bz_Az + #
                 t(MU_EM - MU_GM*mu_GE/mu_GG) %*% BETA_M + t(MU_GW*mu_GE/mu_GG - MU_EW) %*% ALPHA_W ) /#
    (mu_EE - mu_GE^2/mu_GG)#
  Bg_Ag <- ( alpha_E*mu_GE - beta_E*mu_Gf + alpha_I*mu_GGE - beta_I*mu_GGh - t(MU_GZ) %*% Bz_Az + #
               t(MU_GW) %*% ALPHA_W - t(MU_GM) %*% BETA_M ) / mu_GG#
  alpha_G <- beta_G - Bg_Ag#
  alpha_0 <- beta_0 + beta_E*mu_f + beta_I*mu_Gh - alpha_I*mu_GE + t(MU_Z) %*% Bz_Az +#
    t(MU_M) %*% BETA_M - t(MU_W) %*% ALPHA_W#
  # Return #
  return(list(alpha_list=list(alpha_0, alpha_G, alpha_E, alpha_I, ALPHA_Z, ALPHA_W),#
              beta_list = list(beta_0, beta_G, beta_E, beta_I, BETA_Z, BETA_M),#
              cov_list = list(mu_GG, mu_GE, mu_Gf, mu_Gh, MU_GZ, MU_GM, MU_GW, mu_EE,#
              mu_Ef, MU_EZ, MU_EM, MU_EW, MU_fZ, MU_fW),#
              cov_mat_list = list(MU_ZZ, MU_WW, MU_ZW, MU_WZ, MU_ZM, MU_WM),#
              mu_list = list(mu_f, mu_h, rep(0,num_Z), MU_M, rep(0,num_W)),#
              HOM_list = list(mu_GGE, mu_GGh, mu_GEE, mu_GEf, mu_GEh, MU_GEZ, MU_GEM, MU_GEW,#
              MU_GhW, MU_GhZ, mu_GGEE, mu_GGEf, mu_GGEh)))#
}
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
#' GE_translate_inputs_old.R#
#'#
#' Mostly for internal use, function called by GE_bias_normal_old() and GE_scoreeq_sim_old()#
#' to translate the rho_list inputs and return a total covariance matrix for simulation/#
#' checking validity of covariance structure.  If invalid covariance structure, will stop#
#' and return an error message.#
#' #
#' @param rho_list A list of the 6 pairwise covariances between the#
#' covariates.  These should be in the order (1) cov_GE (2) cov_GZ (3) cov_EZ#
#' (4) cov_GW (5) cov_EW (6) cov_ZW. If Z or M are vectors then terms like cov_GZ should be vectors #
#' (in the appropriate order).#
#' If Z or M are vectors, then cov_ZW should be a vector in the order (cov(Z_1,W_1),...,cov(Z_1,W_q),#
#' cov(Z_2,W_1),........,cov(Z_p,W_q) where Z is a vector of length p and W is a vector of length q.#
#' @param cov_Z Only used if Z is a vector, gives the covariance matrix of Z (remember by assumption#
#' Z has mean 0 and variance 1).  The (i,j) element of the matrix should be the (i-1)(i-2)/2+j element#
#' of the vector.#
#' @param cov_W Only used if W is a vector, gives the covariance matrix of W (remember by assumption#
#' W has mean 0 and variance 1).  The (i,j) element of the matrix should be the (i-1)(i-2)/2+j element#
#' of the vector.#
#' @param prob_G Probability that each allele is equal to 1.  Since each SNP has#
#' two alleles, the expectation of G is 2*prob_G.#
#'#
#' @return A list with the elements:#
#' \item{sig_mat_total}{The sigma parameter for rmvnorm call to generate our data.}#
#' \item{sig_mat_ZZ}{The covariance matrix of Z, i.e. E[ZZ^T]}#
#' \item{sig_mat_WW}{The covariance matrix of W, i.e. E[WW^T]}#
#'#
#' @keywords internal#
#' @export#
#' @examples #
#' GE_translate_inputs_old( beta_list=as.list(runif(n=6, min=0, max=1)), #
#'							rho_list=as.list(rep(0.3,6)), prob_G=0.3)#
#
GE_translate_inputs_old <- function(beta_list, rho_list, prob_G, cov_Z=NULL, cov_W=NULL)#
{#
	  # First, make sure we got good inputs#
  	if (length(beta_list) != 6 | length(rho_list) != 6 | class(beta_list) != 'list' | class(rho_list) != 'list')#
  	{#
  	  stop('Input vectors not the right size!')#
  	}#
#
	  # How long are these vectors?  Remember that W is the same length as M by assumption.#
    # Check for Z and M/W nonexistence LATER#
    num_Z <- length(beta_list[[5]])#
    num_W <- length(beta_list[[6]])#
#
  	# Make sure we have compatible lengths for rho_list#
    if (length(rho_list[[2]]) != num_Z | length(rho_list[[3]]) != num_Z | length(rho_list[[4]]) != num_W#
   			| length(rho_list[[5]]) != num_W | length(rho_list[[6]]) != num_Z*num_W) {#
   		stop('Incompatible number of elements in beta/rho_list')#
   	}#
   	# Fill in our covariances.#
   	rho_GE <- rho_list[[1]]; rho_GZ <- rho_list[[2]]; rho_EZ <- rho_list[[3]]#
   	rho_GW <- rho_list[[4]]; rho_EW <- rho_list[[5]]; rho_ZW <- rho_list[[6]]#
   	# Now check for Z and M/W nonexistence, after confirming rho_list is correct length#
   	# If no Z or M/W, then we need to set num_Z and num_W=0 in this function so that#
   	# the indicies will be correct later for building the covariance matrix.#
   	if (length(beta_list[[5]]) == 1 & beta_list[[5]][1] == 0) { num_Z <- 0 }#
   	if (length(beta_list[[6]]) == 1 & beta_list[[6]][1] == 0) { num_W <- 0 }#
   	#################################################################
    # Build our covariance matrix in steps.#
    # The 3x3 in the top left is always the same to build, vectors or not.#
    w <- qnorm(1-prob_G)					# Threshold for generating G#
    r_GE <- rho_GE / (2*dnorm(w))	#
    sig_mat_GE <- matrix(data=c(1, 0, r_GE, 0, 1, r_GE, r_GE, r_GE, 1), nrow=3) #
    # Build the p*3 matrix that describes Z with G1,G2,E#
    if (num_Z != 0) {#
      sig_mat_Z_column <- matrix(data=NA, nrow=num_Z, ncol=3)#
      r_GZ <- rho_GZ / (2*dnorm(w))#
      for (i in 1:num_Z) {#
    	  sig_mat_Z_column[i,] <- c(r_GZ[i], r_GZ[i], rho_EZ[i])#
      }#
    } #
    # Build the q*3 matrix that describes W with G1,G2,E#
    if (num_W != 0)#
    {#
      sig_mat_W_column <- matrix(data=NA, nrow=num_W, ncol=3)#
      r_GW <- rho_GW / (2*dnorm(w))#
      for (i in 1:num_W) {#
      	sig_mat_W_column[i,] <- c(r_GW[i], r_GW[i], rho_EW[i])#
      }#
    } #
    # Build the p*q matrix that describes Z with W.#
    if (num_Z != 0 & num_W != 0) {#
      sig_mat_Z_W <- matrix(data=NA, nrow=num_Z, ncol=num_W)#
      for (i in 1:num_Z) {#
    	  start_ind <- (i-1)*num_W+1#
    	  end_ind <- i*num_W#
    	  sig_mat_Z_W[i,] <- rho_ZW[start_ind:end_ind]#
      }#
    }  # No else here, just don't build it later.#
    # If Z or W vectorized, build the ZZ and WW covariance matrices too#
    if (num_Z > 1) {#
    	sig_mat_ZZ <- matrix(data=0, nrow=num_Z, ncol=num_Z)#
    	sig_mat_ZZ[upper.tri(sig_mat_ZZ)] <- cov_Z#
    	sig_mat_ZZ <- sig_mat_ZZ + t(sig_mat_ZZ)#
    	diag(sig_mat_ZZ) <- 1#
    } else if (num_Z == 0) {        # If no Z, then return NULL for sig_mat_ZZ.#
      sig_mat_ZZ <- NULL#
    } else if (num_Z == 1) {#
    	sig_mat_ZZ <- matrix(data=1, nrow=1, ncol=1) #
    }#
    if (num_W > 1) {#
    	sig_mat_WW <- matrix(data=0, nrow=num_W, ncol=num_W)#
    	sig_mat_WW[upper.tri(sig_mat_WW)] <- cov_W#
    	sig_mat_WW <- sig_mat_WW + t(sig_mat_WW)#
    	diag(sig_mat_WW) <- 1#
    } else if (num_W == 0) {      # If no W, then return NULL for sig_mat_WW.#
      sig_mat_WW <- NULL#
    } else if (num_W == 1) {#
    	sig_mat_WW <- matrix(data=1, nrow=1, ncol=1)#
    }#
    # Now put it all together.#
    # Top left 3x3 always there.#
    sig_mat_total <- matrix(data=NA, nrow=(3+num_Z+num_W), ncol=(3+num_Z+num_W))#
    sig_mat_total[1:3, 1:3] <- sig_mat_GE#
    # If we have a Z#
    if (num_Z > 0) {#
      sig_mat_total[4:(3+num_Z), 1:3] <- sig_mat_Z_column#
      sig_mat_total[1:3, 4:(3+num_Z)] <- t(sig_mat_Z_column)#
      sig_mat_total[4:(3+num_Z), 4:(3+num_Z)] <- sig_mat_ZZ#
    }#
    # If we have a W#
    if (num_W > 0) {#
      sig_mat_total[(4+num_Z):(3+num_Z+num_W), 1:3] <- sig_mat_W_column#
      sig_mat_total[1:3, (4+num_Z):(3+num_Z+num_W)] <- t(sig_mat_W_column)#
      sig_mat_total[(4+num_Z):(3+num_Z+num_W), (4+num_Z):(3+num_Z+num_W)] <- sig_mat_WW#
    }#
    # If we have both Z and W#
    if (num_Z > 0 & num_W > 0)#
    {#
      sig_mat_total[4:(3+num_Z), (4+num_Z):(3+num_Z+num_W)] <- sig_mat_Z_W#
      sig_mat_total[(4+num_Z):(3+num_Z+num_W), 4:(3+num_Z)] <- t(sig_mat_Z_W)#
    }#
    # Final sanity check that the building went correctly.#
    if (!isSymmetric(sig_mat_total)) {stop("Problem building covariance matrix!")}#
    # Now make sure we can actually generate data with this structure#
    test_data <- tryCatch(mvtnorm::rmvnorm(n=1, sigma=sig_mat_total), #
    				warning=function(w) w, error=function(e) e)#
    if (class(test_data)[1] != 'matrix') {stop('You specified an impossible covariance matrix!')}#
    return(list(sig_mat_total=sig_mat_total, sig_mat_ZZ=sig_mat_ZZ, sig_mat_WW=sig_mat_WW))#
}
##################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}
########################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
########################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#####################################################################################
#
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)#
unlist(solve_results$alpha_list)[c(2,4)]
set.seed(0)
set.seed(seed)#
  num_Z <- ceiling(runif(n=1, min=2, max=5))#
  num_W <- ceiling(runif(n=1, min=2, max=5))#
  half_Z <- ceiling(num_Z/2)#
  half_W <- ceiling(num_W/2)#
  beta_list <- list( runif(n=1, min=0.1, max=1),#
                     runif(n=1, min=0.1, max=1),#
                     runif(n=1, min=0.1, max=1),#
                     beta_I,#
                     runif(n=num_Z, min=0.1, max=1),#
                     runif(n=num_W, min=0.1, max=1)	)#
  rho_list <- list( 0,#
                    c( rep(0, half_Z), runif(n=(num_Z-half_Z), min=0.1, max=0.3) ),#
                    c( runif(n=half_Z, min=0.1, max=0.3), rep(0, num_Z-half_Z) ),#
                    c( rep(0, half_W), runif(n=(num_W-half_W), min=0.1, max=0.3) ),#
                    c( runif(n=half_W, min=0.1, max=0.3), rep(0, num_W-half_W) ),#
                    runif(n=num_W*num_Z, min=0.02, max=0.3) )#
  # MAF#
  prob_G = runif(n=1, min=0.05, max=0.95)#
  # Covariance matrices for Z and W#
  if (num_Z > 1) {#
    temp <- num_Z*(num_Z-1) / 2#
    cov_Z <- runif(n=temp, min=0.02, max=0.2)#
  } else {#
    cov_Z <- NULL#
  }#
  if (num_W > 1) {#
    temp <- num_W*(num_W-1) / 2#
    cov_W <- runif(n=temp, min=0.02, max=0.2)#
  } else {#
    cov_W <- NULL#
  }
beta_list
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}#
#
# Show the parameters#
beta_list#
prob_G#
#
# Test all functions#
# First make normal + squared_mis assumptions#
solve_results <- GE_bias_normal_squaredmis_old(beta_list, rho_list, prob_G, cov_Z, cov_W)
beta_list
set.seed(0)#
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))
num_Z
num_W
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)#
if (num_Z > 1) {#
	temp <- num_Z*(num_Z-1) / 2#
	cov_Z <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_Z <- NULL#
}#
if (num_W > 1) {#
	temp <- num_W*(num_W-1) / 2#
	cov_W <- runif(n=temp, min=0.02, max=0.2)#
} else {#
	cov_W <- NULL#
}
prob_G
set.seed(0)
runif(1)
set.seed(0)#
beta_I = 0
# Test another result - which betas do \alpha_G and \alpha_I depend on?#
num_Z <- ceiling(runif(n=1, min=2, max=5))#
num_W <- ceiling(runif(n=1, min=2, max=5))#
beta_list <- list( runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				runif(n=1, min=0.1, max=1),#
				beta_I,#
				runif(n=num_Z, min=0.1, max=1),#
				runif(n=num_W, min=0.1, max=1)	)#
rho_list <- list( runif(n=1, min=0.1, max=1),#
					 c( runif(n=num_Z, min=0.1, max=0.3) ),#
					  c( runif(n=num_Z, min=0.1, max=0.3) ),#
					   c( runif(n=num_W, min=0.1, max=0.3) ),#
					    c( runif(n=num_W, min=0.1, max=0.3) ),#
					     runif(n=num_W*num_Z, min=0.02, max=0.3) )#
prob_G = runif(n=1, min=0.05, max=0.95)
prob_G
beta_list
rho_list
